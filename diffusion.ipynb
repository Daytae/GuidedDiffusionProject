{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "005a861f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\2023r\\Documents\\GuidedDiffusionProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from load_and_sample import *\n",
    "from guided_diffusion import guided_diffusion_1d\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7d413d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n",
      "loading model from ./saved_models/epoch=447-step=139328.ckpt\n",
      "Enc params: 1,994,592\n",
      "Dec params: 277,346\n",
      "Created model & loaded data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4219:   0%|          | 202/100000 [00:16<2:12:50, 12.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreated model & loaded data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m trainer = guided_diffusion_1d.Trainer1D(\n\u001b[32m     69\u001b[39m     diffusion_model=model,\n\u001b[32m     70\u001b[39m     dataset = latents_dataset,\n\u001b[32m   (...)\u001b[39m\u001b[32m     75\u001b[39m     num_workers=\u001b[32m0\u001b[39m\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\2023r\\Documents\\GuidedDiffusionProject\\guided_diffusion\\guided_diffusion_1d.py:932\u001b[39m, in \u001b[36mTrainer1D.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    929\u001b[39m         loss = loss / \u001b[38;5;28mself\u001b[39m.gradient_accumulate_every\n\u001b[32m    930\u001b[39m         total_loss += loss.item()\n\u001b[32m--> \u001b[39m\u001b[32m932\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    934\u001b[39m pbar.set_description(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    936\u001b[39m accelerator.wait_for_everyone()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\2023r\\Documents\\GuidedDiffusionProject\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:2454\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2452\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2453\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2454\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\2023r\\Documents\\GuidedDiffusionProject\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\2023r\\Documents\\GuidedDiffusionProject\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\2023r\\Documents\\GuidedDiffusionProject\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from load_and_sample import *\n",
    "from guided_diffusion import guided_diffusion_1d\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using: {device}\")\n",
    "\n",
    "# Read from the latent data file and put it into a dataloader\n",
    "\n",
    "class LatentDataset(Dataset):\n",
    "    def __init__(self, latents):\n",
    "        self.latents = torch.from_numpy(latents).float().unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latents)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.latents[idx]\n",
    "\n",
    "# Initialize the diffusion model\n",
    "\n",
    "def create_diffusion_model(unet_dim=128, latent_dim=128, num_timesteps=1000):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    unet_model = guided_diffusion_1d.Unet1D(\n",
    "        dim = unet_dim,\n",
    "        channels=1,\n",
    "        dim_mults=(1, 2, 4, 8)\n",
    "    ).to(device)\n",
    "\n",
    "    diffusion_model = guided_diffusion_1d.GaussianDiffusion1D(\n",
    "        unet_model,\n",
    "        seq_length=latent_dim,\n",
    "        timesteps=num_timesteps,\n",
    "        objective='pred_v'\n",
    "    ).to(device)\n",
    "\n",
    "    return diffusion_model\n",
    "\n",
    "def sample_diffusion(diffusion_model, sample_batch_size=4, latent_dim=128):\n",
    "    diffusion_model.eval()\n",
    "    with torch.no_grad():\n",
    "        latents = diffusion_model.sample(batch_size=sample_batch_size)\n",
    "        latents = latents.reshape(sample_batch_size, latent_dim)\n",
    "        return latents\n",
    "    \n",
    "# load vae\n",
    "vae = load_vae_selfies(\"./saved_models/epoch=447-step=139328.ckpt\")\n",
    "\n",
    "# dataset\n",
    "latents = np.load(\"latents_final.npy\")\n",
    "latents_dataset = LatentDataset(latents=latents)\n",
    "# latents_dataloader = DataLoader(latents_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# load and train model\n",
    "model = create_diffusion_model(unet_dim=128, latent_dim=128, num_timesteps=1000)\n",
    "\n",
    "print(\"Created model & loaded data\")\n",
    "\n",
    "trainer = guided_diffusion_1d.Trainer1D(\n",
    "    diffusion_model=model,\n",
    "    dataset = latents_dataset,\n",
    "    train_batch_size=32,\n",
    "    save_and_sample_every=10000,\n",
    "    num_samples=16,\n",
    "    results_folder='./diffusion_results',\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a4c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img display\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image as mpimg\n",
    "import io\n",
    "def display_molecule(smiles_string, title=None):\n",
    "    \"\"\"\n",
    "    Display molecular structure from SMILES string\n",
    "    \n",
    "    Args:\n",
    "        smiles_string (str): SMILES notation of the molecule\n",
    "        title (str): Optional title for the plot\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse SMILES string\n",
    "        mol = Chem.MolFromSmiles(smiles_string)\n",
    "        \n",
    "        if mol is None:\n",
    "            print(f\"Error: Invalid SMILES string '{smiles_string}'\")\n",
    "            return\n",
    "        \n",
    "        # Generate 2D coordinates for better visualization\n",
    "        from rdkit.Chem import rdDepictor\n",
    "        rdDepictor.Compute2DCoords(mol)\n",
    "        \n",
    "        # Create molecular image\n",
    "        img = Draw.MolToImage(mol, size=(400, 400))\n",
    "        \n",
    "        # Convert PIL image to numpy array for matplotlib\n",
    "        img_array = mpimg.pil_to_array(img)\n",
    "        \n",
    "        # Display the image\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img_array)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if title:\n",
    "            plt.title(title, fontsize=14, fontweight='bold')\n",
    "        else:\n",
    "            plt.title(f'Molecule: {smiles_string}', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print molecule information\n",
    "        print(f\"SMILES: {smiles_string}\")\n",
    "        # print(f\"Molecular Formula: {Chem.rdMolDescriptors.CalcMolFormula(mol)}\")\n",
    "        # print(f\"Molecular Weight: {Chem.rdMolDescriptors.CalcExactMolWt(mol):.2f}\")\n",
    "        # print(f\"Number of Atoms: {mol.GetNumAtoms()}\")\n",
    "        # print(f\"Number of Bonds: {mol.GetNumBonds()}\")\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(\"Error: Required packages not installed.\")\n",
    "        print(\"Install with: pip install rdkit matplotlib\")\n",
    "        print(f\"Details: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing molecule: {e}\")\n",
    "\n",
    "def display_diffusion_sample(latent_batch, vae):\n",
    "    selfies = latent_to_selfies_batch(latent_batch, vae=vae)\n",
    "    for selfie in selfies:\n",
    "        display_molecule(sf.decoder(selfie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4dfcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from ./saved_models/epoch=447-step=139328.ckpt\n",
      "Enc params: 1,994,592\n",
      "Dec params: 277,346\n",
      "Created model & loaded data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.6177:   0%|          | 155/100000 [00:12<2:12:46, 12.53it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreated model & loaded data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m trainer = guided_diffusion_1d.Trainer1D(\n\u001b[32m     15\u001b[39m     diffusion_model=model,\n\u001b[32m     16\u001b[39m     dataset = latents_dataset,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     num_workers=\u001b[32m0\u001b[39m\n\u001b[32m     22\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\2023r\\Documents\\GuidedDiffusionProject\\guided_diffusion\\guided_diffusion_1d.py:932\u001b[39m, in \u001b[36mTrainer1D.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    929\u001b[39m         loss = loss / \u001b[38;5;28mself\u001b[39m.gradient_accumulate_every\n\u001b[32m    930\u001b[39m         total_loss += loss.item()\n\u001b[32m--> \u001b[39m\u001b[32m932\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    934\u001b[39m pbar.set_description(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    936\u001b[39m accelerator.wait_for_everyone()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\2023r\\Documents\\GuidedDiffusionProject\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:2454\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2452\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2453\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2454\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\2023r\\Documents\\GuidedDiffusionProject\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\2023r\\Documents\\GuidedDiffusionProject\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\2023r\\Documents\\GuidedDiffusionProject\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
