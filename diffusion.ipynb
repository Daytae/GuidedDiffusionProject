{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "005a861f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\2023r\\Documents\\GuidedDiffusionProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from load_and_sample import *\n",
    "from guided_diffusion import guided_diffusion_1d\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07caaffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the latent data file and put it into a dataloader\n",
    "\n",
    "class LatentDataset(Dataset):\n",
    "    def __init__(self, latents):\n",
    "        self.latents = torch.from_numpy(latents).float().unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latents)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.latents[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7d413d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the diffusion model\n",
    "\n",
    "def create_diffusion_model(unet_dim=128, latent_dim=128, num_timesteps=1000):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    unet_model = guided_diffusion_1d.Unet1D(\n",
    "        dim = unet_dim,\n",
    "        channels=1,\n",
    "        dim_mults=(1, 2, 4, 8)\n",
    "    ).to(device)\n",
    "\n",
    "    diffusion_model = guided_diffusion_1d.GaussianDiffusion1D(\n",
    "        unet_model,\n",
    "        seq_length=latent_dim,\n",
    "        timesteps=num_timesteps,\n",
    "        objective='pred_v'\n",
    "    ).to(device)\n",
    "\n",
    "    return diffusion_model\n",
    "\n",
    "def sample_diffusion(diffusion_model, sample_batch_size=4, latent_dim=128):\n",
    "    diffusion_model.eval()\n",
    "    with torch.no_grad():\n",
    "        latents = diffusion_model.sample(batch_size=sample_batch_size)\n",
    "        latents = latents.reshape(sample_batch_size, latent_dim)\n",
    "        return latents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29a4c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img display\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image as mpimg\n",
    "import io\n",
    "def display_molecule(smiles_string, title=None):\n",
    "    \"\"\"\n",
    "    Display molecular structure from SMILES string\n",
    "    \n",
    "    Args:\n",
    "        smiles_string (str): SMILES notation of the molecule\n",
    "        title (str): Optional title for the plot\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse SMILES string\n",
    "        mol = Chem.MolFromSmiles(smiles_string)\n",
    "        \n",
    "        if mol is None:\n",
    "            print(f\"Error: Invalid SMILES string '{smiles_string}'\")\n",
    "            return\n",
    "        \n",
    "        # Generate 2D coordinates for better visualization\n",
    "        from rdkit.Chem import rdDepictor\n",
    "        rdDepictor.Compute2DCoords(mol)\n",
    "        \n",
    "        # Create molecular image\n",
    "        img = Draw.MolToImage(mol, size=(400, 400))\n",
    "        \n",
    "        # Convert PIL image to numpy array for matplotlib\n",
    "        img_array = mpimg.pil_to_array(img)\n",
    "        \n",
    "        # Display the image\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img_array)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if title:\n",
    "            plt.title(title, fontsize=14, fontweight='bold')\n",
    "        else:\n",
    "            plt.title(f'Molecule: {smiles_string}', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print molecule information\n",
    "        print(f\"SMILES: {smiles_string}\")\n",
    "        # print(f\"Molecular Formula: {Chem.rdMolDescriptors.CalcMolFormula(mol)}\")\n",
    "        # print(f\"Molecular Weight: {Chem.rdMolDescriptors.CalcExactMolWt(mol):.2f}\")\n",
    "        # print(f\"Number of Atoms: {mol.GetNumAtoms()}\")\n",
    "        # print(f\"Number of Bonds: {mol.GetNumBonds()}\")\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(\"Error: Required packages not installed.\")\n",
    "        print(\"Install with: pip install rdkit matplotlib\")\n",
    "        print(f\"Details: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing molecule: {e}\")\n",
    "\n",
    "def display_diffusion_sample(latent_batch, vae):\n",
    "    selfies = latent_to_selfies_batch(latent_batch, vae=vae)\n",
    "    for selfie in selfies:\n",
    "        display_molecule(sf.decoder(selfie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af4dfcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from ./saved_models/epoch=447-step=139328.ckpt\n",
      "Enc params: 1,994,592\n",
      "Dec params: 277,346\n",
      "Created model & loaded data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:16<00:00, 62.13it/s]\n",
      "loss: 0.4809:   0%|          | 9/100000 [00:17<53:43:49,  1.93s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 16), |u1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\2023r\\Documents\\GuidedDiffusionProject\\.venv\\Lib\\site-packages\\PIL\\Image.py:3311\u001b[39m, in \u001b[36mfromarray\u001b[39m\u001b[34m(obj, mode)\u001b[39m\n\u001b[32m   3310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3311\u001b[39m     mode, rawmode = \u001b[43m_fromarray_typemap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtypekey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   3312\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyError\u001b[39m: ((1, 1, 16), '|u1')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreated model & loaded data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m trainer = guided_diffusion_1d.Trainer1D(\n\u001b[32m     15\u001b[39m     diffusion_model=model,\n\u001b[32m     16\u001b[39m     dataset = latents_dataset,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     num_workers=\u001b[32m0\u001b[39m\n\u001b[32m     22\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\2023r\\Documents\\GuidedDiffusionProject\\guided_diffusion\\guided_diffusion_1d.py:957\u001b[39m, in \u001b[36mTrainer1D.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    954\u001b[39m             all_samples_list = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m n: \u001b[38;5;28mself\u001b[39m.ema.ema_model.sample(batch_size=n), batches))\n\u001b[32m    956\u001b[39m         all_samples = torch.cat(all_samples_list, dim = \u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m         \u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresults_folder\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msample-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmilestone\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.png\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnrow\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m         \u001b[38;5;28mself\u001b[39m.save(milestone)\n\u001b[32m    960\u001b[39m pbar.update(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\2023r\\Documents\\GuidedDiffusionProject\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\2023r\\Documents\\GuidedDiffusionProject\\.venv\\Lib\\site-packages\\torchvision\\utils.py:150\u001b[39m, in \u001b[36msave_image\u001b[39m\u001b[34m(tensor, fp, format, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\u001b[39;00m\n\u001b[32m    149\u001b[39m ndarr = grid.mul(\u001b[32m255\u001b[39m).add_(\u001b[32m0.5\u001b[39m).clamp_(\u001b[32m0\u001b[39m, \u001b[32m255\u001b[39m).permute(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m).to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, torch.uint8).numpy()\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m im = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mndarr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m im.save(fp, \u001b[38;5;28mformat\u001b[39m=\u001b[38;5;28mformat\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\2023r\\Documents\\GuidedDiffusionProject\\.venv\\Lib\\site-packages\\PIL\\Image.py:3315\u001b[39m, in \u001b[36mfromarray\u001b[39m\u001b[34m(obj, mode)\u001b[39m\n\u001b[32m   3313\u001b[39m         typekey_shape, typestr = typekey\n\u001b[32m   3314\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot handle this data type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypekey_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypestr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3315\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   3316\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3317\u001b[39m     rawmode = mode\n",
      "\u001b[31mTypeError\u001b[39m: Cannot handle this data type: (1, 1, 16), |u1"
     ]
    }
   ],
   "source": [
    "# load vae\n",
    "vae = load_vae_selfies(\"./saved_models/epoch=447-step=139328.ckpt\")\n",
    "\n",
    "# dataset\n",
    "latents = np.load(\"latents_final.npy\")\n",
    "latents_dataset = LatentDataset(latents=latents)\n",
    "# latents_dataloader = DataLoader(latents_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# load and train model\n",
    "model = create_diffusion_model(unet_dim=128, latent_dim=128, num_timesteps=1000)\n",
    "\n",
    "print(\"Created model & loaded data\")\n",
    "\n",
    "trainer = guided_diffusion_1d.Trainer1D(\n",
    "    diffusion_model=model,\n",
    "    dataset = latents_dataset,\n",
    "    train_batch_size=32,\n",
    "    save_and_sample_every=10,\n",
    "    num_samples=16,\n",
    "    results_folder='./diffusion_results',\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
