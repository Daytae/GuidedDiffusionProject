{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from peptidevae.load_vae import load_vae, vae_decode, vae_forward\n",
    "from importlib import reload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_csv = \"/Users/aldenrose/DiffusionProject/small_data.csv\"\n",
    "big_csv = \"/Users/aldenrose/DiffusionProject/combined_data.csv\"\n",
    "VAE_PKL_LOCATION = \"/Users/aldenrose/DiffusionProject/peptidevae/checkpoints/dim128_k1_kl0001_eff256_dff256_pious-sea-2_model_state_epoch_118.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNET_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae, dataobj = load_vae(VAE_PKL_LOCATION, dim=256, max_string_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding peptides: 100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from data import PeptideLatentDataset\n",
    "import data\n",
    "reload(data)\n",
    "ds = PeptideLatentDataset(small_csv, vae, dataobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m latents = \u001b[43mtorch\u001b[49m.tensor(pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mlatents.csv\u001b[39m\u001b[33m'\u001b[39m).to_numpy(dtype=\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      2\u001b[39m labels = torch.tensor(pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mlabels.csv\u001b[39m\u001b[33m'\u001b[39m).to_numpy(dtype=\u001b[33m'\u001b[39m\u001b[33mint8\u001b[39m\u001b[33m'\u001b[39m).squeeze(-\u001b[32m1\u001b[39m))\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# for debugging\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "latents = torch.tensor(pd.read_csv('latents.csv').to_numpy(dtype='float32'))\n",
    "labels = torch.tensor(pd.read_csv('labels.csv').to_numpy(dtype='int8').squeeze(-1))\n",
    "# for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('combined_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = df['sequence'].to_numpy()\n",
    "labels = df['extinct'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    latents, vae_loss = vae_forward(seq[:1000], dataobj, vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EsmClassificationHead(nn.Module):\n",
    "    # slightly modified from the original ESM classification head\n",
    "    def __init__(self, input_dim=256):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, 2048)\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.dense2 = nn.Linear(2048, 2048)\n",
    "        self.dense3 = nn.Linear(2048, 2048)\n",
    "        self.out_proj = nn.Linear(2048, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense3(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path='best_model.pt'):\n",
    "    # Check for GPU availability\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load saved model data\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Initialize model with saved input dimension\n",
    "    model = EsmClassificationHead(input_dim=256).to(device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, device\n",
    "\n",
    "def predict_classifer(model, embeddings, device, batch_size=100):\n",
    "    \n",
    "    # Convert to torch tensor\n",
    "    embeddings = torch.FloatTensor(embeddings)\n",
    "    predictions = []\n",
    "    all_logits = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        for i in range(0, len(embeddings), batch_size):\n",
    "            batch = embeddings[i:i + batch_size].to(device)\n",
    "            logits = model(batch)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_logits.extend(logits.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    \n",
    "    return (np.array(predictions), \n",
    "            np.array(all_logits), \n",
    "            np.array(all_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "classifier_model, device = load_model(\"/Users/aldenrose/DiffusionProject/classifer/best_model.pt\")\n",
    "embeddings = latents\n",
    "\n",
    "# load labels for validation\n",
    "_labels = labels[:1000]\n",
    "\n",
    "# Get predictions\n",
    "predictions, logits, probabilities = predict_classifer(classifier_model, embeddings, device)\n",
    "\n",
    "accuracy = np.mean(predictions == _labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet output shape: torch.Size([1, 1, 256])\n",
      "Loss: 1.0439468622207642\n"
     ]
    }
   ],
   "source": [
    "from guided_diffusion.guided_diffusion_1d import Unet1D, GaussianDiffusion1D\n",
    "seq_length = 256\n",
    "model = Unet1D(dim=seq_length, channels=1, self_condition=False)\n",
    "\n",
    "# input dummy tensors\n",
    "batch_size = 1\n",
    "channels = 1    # same as model channels\n",
    "dummy_input = torch.randn(batch_size, channels, seq_length)\n",
    "\n",
    "dummy_time = torch.randint(0, 1000, (batch_size,)).float()  # timesteps in [0, num_timesteps)\n",
    "\n",
    "# check pass works\n",
    "output = model(dummy_input, dummy_time)\n",
    "print(\"UNet output shape:\", output.shape)\n",
    "\n",
    "diffusion_model = GaussianDiffusion1D(\n",
    "    model=model,\n",
    "    seq_length=seq_length,\n",
    "    timesteps=500\n",
    ")\n",
    "\n",
    "loss = diffusion_model(dummy_input) \n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# dummy_classifier = Classifier(seq_length=seq_length, num_classes=2)\n",
    "# dummy_labels = torch.randint(0, 1, (batch_size,))\n",
    "# grad = classifier_cond_fn(dummy_input, dummy_time, dummy_classifier, dummy_labels, classifier_scale=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 500/500 [01:18<00:00,  6.38it/s]\n"
     ]
    }
   ],
   "source": [
    "sampled = diffusion_model.sample(batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = sampled.reshape(-1, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, logits, probabilities = predict_classifer(classifier_model, sampled, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
